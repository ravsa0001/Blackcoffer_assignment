{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdb63fe1-369c-4a00-b293-bfd7a81ca676",
   "metadata": {},
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ccee8f-da15-462c-af77-c2d9f6e0d535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082849e-4b00-4fc3-a8d8-3395d04afee1",
   "metadata": {},
   "source": [
    "# Reading the input file into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ebe7b8-5392-4164-a237-c0f5b9d30c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"Input.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4908dab6-fbd2-4267-bd12-bbe3da300026",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a483baf-4849-4513-9998-f16ef5ccd4db",
   "metadata": {},
   "source": [
    "# Extractint the article text and saving it inside a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b26b29-8706-46ac-a801-d6e48c5b5e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (url, filename) in zip(data[\"URL\"], data[\"URL_ID\"]):\n",
    "    # try:\n",
    "        file_name = filename + \".txt\"\n",
    "        file_path = \"/home/vo1d/Desktop/VS_Code/Blackcoffer/files/\" + file_name\n",
    "        # print(file_name)\n",
    "        content = requests.get(url)\n",
    "        soup = BeautifulSoup(content.text, \"html.parser\")  \n",
    "\n",
    "        # title_element = soup.find(\"h1\", attrs={'class': 'entry-title'})\n",
    "        # content_element = soup.find(\"div\", attrs={'class': 'td-post-content tagdiv-type'})\n",
    "        article_title = soup.find(\"h1\", attrs = {\"class\": \"entry-title\"}).get_text()\n",
    "        article_text = soup.find(class_ = \"td-post-content tagdiv-type\").get_text()\n",
    "    \n",
    "        total_text = article_title + \"\\n\\n\" + article_text\n",
    "        \n",
    "        with open(file_path, \"w\") as f:\n",
    "            f.write(total_text)\n",
    "    # except AttributionError as e:\n",
    "    #     print(\"Error in {}\".format(file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef929b6b-3987-4283-91a3-fcd860d31829",
   "metadata": {},
   "source": [
    "# Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50438daf-b8dd-4165-911b-062458e8bbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_dir = \"/home/vo1d/Desktop/VS_Code/Blackcoffer/stopwords/\"\n",
    "sentiment_dir = \"/home/vo1d/Desktop/VS_Code/Blackcoffer/sentiment_docs/\"\n",
    "text_dir = \"/home/vo1d/Desktop/VS_Code/Blackcoffer/files/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81437ea9-2599-4583-b3c5-5497d735bd19",
   "metadata": {},
   "source": [
    "# Storing the stopwrods in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fea726c-e0fb-4dc3-9352-4a3feae6bf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "for files in os.listdir(stopwords_dir):\n",
    "  with open(os.path.join(stopwords_dir, files), \"r\", encoding = \"ISO-8859-1\") as f:\n",
    "    stop_words.extend(list(f.read().splitlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e468d7b-ab7c-4a28-a567-3efceb32f741",
   "metadata": {},
   "source": [
    "# Storing the positive and negative words in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da822c44-275a-43d0-8f9f-37b3344dab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = []\n",
    "neg = []\n",
    "for files in os.listdir(sentiment_dir):\n",
    "  if files =='positive-words.txt':\n",
    "    with open(os.path.join(sentiment_dir, files), \"r\", encoding = \"ISO-8859-1\") as f:\n",
    "      pos.extend(f.read().splitlines())\n",
    "  else:\n",
    "    with open(os.path.join(sentiment_dir, files), \"r\", encoding = \"ISO-8859-1\") as f:\n",
    "      neg.extend(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e568638-842b-48db-a61d-be01c02193dc",
   "metadata": {},
   "source": [
    "# Storing the article text in a list from the txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7ff1c5-eeec-4b1a-8563-ce8445346912",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list()\n",
    "for files in os.listdir(text_dir):\n",
    "    with open(os.path.join(text_dir, files), \"r\", encoding = \"ISO-8859-1\") as f:\n",
    "        text = (f.read().split())\n",
    "\n",
    "        filtered_text = [words for words in text if words.lower() not in stop_words]\n",
    "        data.append(filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0065ce8-b5ea-43f8-b45a-52212fc301f1",
   "metadata": {},
   "source": [
    "# Calculating the Scores(Positive, Negative, Polarity, Subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acb9113-92cd-4585-b024-84d9bb4fa286",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = []\n",
    "negative_words =[]\n",
    "positive_score = []\n",
    "negative_score = []\n",
    "polarity_score = []\n",
    "subjectivity_score = []\n",
    "\n",
    "for i in range(0, len(data)):\n",
    "    positive_words.append([word for word in data[i] if word.lower() in pos])\n",
    "    negative_words.append([word for word in data[i] if word.lower() in neg])\n",
    "    positive_score.append(len(positive_words[i]))\n",
    "    negative_score.append(len(negative_words[i]))\n",
    "    polarity_score.append((positive_score[i] - negative_score[i]) / (positive_score[i] + negative_score[i]) + 0.000001)\n",
    "    subjectivity_score.append((positive_score[i] + negative_score[i]) / (len(data[i]) + 0.000001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4526a418-7f25-4c4b-b5d0-59dcbe9607bd",
   "metadata": {},
   "source": [
    "# Calculationg AverageSentenceLength, PercentageofComplexWords, FogIndex, SyllablePerWord, ComplexWordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44474bb-a3d2-4671-9504-1e7651e8a907",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sentence_length = []\n",
    "Percentage_of_Complex_words  =  []\n",
    "Fog_Index = []\n",
    "complex_word_count =  []\n",
    "avg_syllable_word_count = []\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "def measure(files):\n",
    "    with open(os.path.join(text_dir, files), \"r\") as f:\n",
    "        text = f.read()\n",
    "        sentence = text.split(\". \")\n",
    "        num_sentences = len(sentence)\n",
    "\n",
    "        words = [ word for word in text.split() if word.lower() not in stopwords ]\n",
    "        num_words = len(words)\n",
    "\n",
    "        complex_words = []\n",
    "        for word in words:\n",
    "            vowels = [\"a\", \"e\", \"i\", \"o\", \"u\"]\n",
    "            syllable_word_count = sum( 1 for letter in word if letter.lower() in vowels)\n",
    "            if syllable_word_count > 2:\n",
    "                complex_words.append(word)\n",
    "\n",
    "        syllable_count = 0\n",
    "        syllable_words = []\n",
    "        for word in words:\n",
    "            if word.endswith(\"es\") or word.endswith(\"ed\"):\n",
    "                pass\n",
    "            else:\n",
    "                vowels = [\"a\", \"e\", \"i\", \"o\", \"u\"]\n",
    "                syllable_count_words = sum(1 for letter in word if letter.lower() in vowels)\n",
    "                if syllable_count_words >= 1:\n",
    "                    syllable_words.append(word)\n",
    "                    syllable_count += syllable_count_words\n",
    "                \n",
    "        avg_sentence_len = num_words / num_sentences\n",
    "        avg_syllable_word_count = syllable_count / len(syllable_words)\n",
    "        Percent_Complex_words  =  len(complex_words) / num_words\n",
    "        Fog_Index = 0.4 * (avg_sentence_len + Percent_Complex_words)\n",
    "\n",
    "    return avg_sentence_len, Percent_Complex_words, Fog_Index, len(complex_words),avg_syllable_word_count\n",
    "\n",
    "\n",
    "for files in os.listdir(text_dir):\n",
    "    x,y,z,a,b = measure(files)\n",
    "    avg_sentence_length.append(x)\n",
    "    Percentage_of_Complex_words.append(y)\n",
    "    Fog_Index.append(z)\n",
    "    complex_word_count.append(a)\n",
    "    avg_syllable_word_count.append(b)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc803be-705d-49ed-8102-0d2f63bc6e8e",
   "metadata": {},
   "source": [
    "# Calculating Word Count, Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83064ec-83d5-4388-a58d-af182c8577e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_words(files):\n",
    "    with open(os.path.join(text_dir, files), \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "        words = [word for word in text.split() if word.lower() not in stopwords]\n",
    "        length = sum(len(word) for word in words)\n",
    "        average_word_length = length / len(words)\n",
    "    \n",
    "    return len(words),average_word_length\n",
    "\n",
    "word_count = []\n",
    "average_word_length = []\n",
    "for file in os.listdir(text_dir):\n",
    "  x, y = cleaned_words(files)\n",
    "  word_count.append(x)\n",
    "  average_word_length.append(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7204e848-58d9-43b4-a136-0c42624d5ca3",
   "metadata": {},
   "source": [
    "# Calculating Personal_Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c185b77-8a61-497b-a683-e2c05d25fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_personal_pronouns(file):\n",
    "  with open(os.path.join(text_dir, file), 'r') as f:\n",
    "    text = f.read()\n",
    "    personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
    "    count = 0\n",
    "    for pronoun in personal_pronouns:\n",
    "      count += len(re.findall(r\"\\b\" + pronoun + r\"\\b\", text)) # \\b is used to match word boundaries\n",
    "  return count\n",
    "\n",
    "pp_count = []\n",
    "for file in os.listdir(text_dir):\n",
    "  x = count_personal_pronouns(file)\n",
    "  pp_count.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92e87b9-14c8-4b94-a6ea-546730fe40af",
   "metadata": {},
   "source": [
    "# Reading the output file into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54472a35-f888-4d20-baf0-6cac07375fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.read_excel(\"Output Data Structure.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d0af1-7989-47aa-ad7f-1268a6817b55",
   "metadata": {},
   "source": [
    "# URL_ID doesn't exists i.e. page doesn't exists, so droping these rows from the dataframe and writing the values to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79585451-611e-447f-9644-5a5db209cdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.drop([13, 19, 28, 35, 42, 48, 82, 83, 91, 98, 99], axis = 0, inplace = True)\n",
    "\n",
    "variables = [positive_score, negative_score, polarity_score, subjectivity_score,\n",
    "            avg_sentence_length, Percentage_of_Complex_words, Fog_Index, avg_sentence_length,\n",
    "            complex_word_count, word_count, avg_syllable_word_count, pp_count, average_word_length]\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "  output_df.iloc[:, i+2] = var\n",
    "\n",
    "# Now save the dataframe to the disk\n",
    "output_df.to_csv('/home/vo1d/Desktop/VS_Code/Blackcoffer/Output_Data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
